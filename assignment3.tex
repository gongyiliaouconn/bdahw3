%%
%% assignment3.tex
%% 
%% Made by GONG-YI LIAO
%% Login   <gong-yi@GongTop>
%% 
%% Started on  Sat Nov  6 17:57:05 2010 GONG-YI LIAO
%%

\documentclass[twoside,11pt]{amsart}

\usepackage{bm,amssymb,amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.0in}
\setlength{\topmargin}{-0.5in}
\setlength{\headheight}{15pt}
\setlength{\headsep}{15pt}
\setlength{\footskip}{25pt}
\setlength{\parindent}{0.25in}
\setlength{\parskip}{0pt} 

\author{Gong-Yi Liao}
\title{Assignment 3}

\begin{document}
\maketitle

\begin{enumerate}
\item
  Consider the model that $(Y_{j1}, Y_{j2}, Y_{j3})'\sim\mathcal
  Multinomial(\bm p_j, \sum_{i=1}^3y_{ji})$, $j=1,2$, $\bm p_j = (p_{j1}, p_{j2},
  p_{j3})'$, $\sum_{i=1}^3p_{ji} = 1$. Apparently $\alpha_j =
  \frac{p_{j1}}{p_{j1}+p_{j2}}$, 
  where $p_{j1}$ denotes the probability that people prefer Bush at
  poll time $j$, , $p_{j2}$ denotes the probability that peoples
  prefer Dukakis at time $j$ and $p_{j3}$ denotes the probabiltiy that
  neither candidates are not preferred by the people. 

  We assume that $\bm p$ has a Dirichlet distribution with parameters
  $\bm\xi = (\xi_1, \xi_2, \xi_3)'$, all $\xi$'s are
  all non-negative real numbers.
  Now, with this prior setting, we have the form:
  \[
  \mathscr P(\bm y_j, \bm p|\bm\xi) =
  \frac{\Gamma\left(\sum_{i=1}^3y_{ji}+1\right)\Gamma\left(\sum_{i=1}^3\xi_i\right)}{\prod_{i=1}^3\Gamma(y_{ji}+1)\cdot\Gamma(\xi_i)}\prod_{i=1}^3p_i^{y_{ji} + \xi_i}
  \]
  and thus
  \[
  \mathscr P(\bm y_j|\bm\xi) = 
  \frac{\Gamma\left(\sum_{i=1}^3y_{ji}+1\right)\Gamma\left(\sum_{i=1}^3\xi_i\right)\prod_{i=1}^3\Gamma(y_{ji}+\xi_i)}
  {\prod_{i=1}^3\left[\Gamma(y_{ji}+1)\cdot\Gamma(\xi_i)\right]\Gamma\left(\sum_{i=1}^3y_{ji}+\xi_i\right)}\times
  \]
  \[
  \int_{\bm p\in (0,1)^3, \sum_{i=1}^3 p_i=1}
  \frac{\Gamma\left(\sum_{i=1}^3y_{ji}+\xi_i\right)}{\prod_{i=1}^3\Gamma(y_{ji}+\xi_i)}
  \prod_{i=1}^3p_i^{y_{ji} + \xi_i}d\bm p
  \]
  \[
  =\frac{\Gamma\left(\sum_{i=1}^3y_{ji}+1\right)\Gamma\left(\sum_{i=1}^3\xi_i\right)\prod_{i=1}^3\Gamma(y_{ji}+\xi_i)}
  {\prod_{i=1}^3\left[\Gamma(y_{ji}+1)\cdot\Gamma(\xi_i)\right]\Gamma\left(\sum_{i=1}^3y_{ji}+\xi_i\right)}
  \]
  and so
  \[
  \mathscr P(\bm p|\bm y_j,\bm\xi) =   \frac{\Gamma\left(\sum_{i=1}^3y_{ji}+\xi_i\right)}{\prod_{i=1}^3\Gamma(y_{ji}+\xi_i)}
  \prod_{i=1}^3p_i^{y_{ji} + \xi_i}
  \]
  Now, a problem raise, if we utilize Dirichlet Distribution, what
  value of $\bm\xi$ we should choose? 
  

  Since we assume (?) that we treat the data {\em subjectively}, so,
  we use $\bm\xi_0 = (1,1,1)'$ as the prior parameter, thus, we have
   \[
    \mathscr P(\bm p|\bm y_j,\bm\xi_0) =
    \frac{\Gamma\left(\sum_{i=1}^3y_{ji} +1\right)}{\prod_{i=1}^3\Gamma(y_{ji}+1)}
  \prod_{i=1}^3p_i^{y_{ji} + \xi_i}
   \]
   thus, $\bm p_1|\bm y_1, \bm\xi_0\sim\text{Dirichlet}(295, 308,
   39)$ and $\bm p_2|\bm y_2, \bm\xi_0\sim\text{Dirichlet}(289, 333,
   20)$. Thus, we can generate the samples of $\bm p_j$ from the
   corresponding posterior distributions and compute $\alpha_j$'s then
   plot their difference. The plot is shown in Fig.\ref{fig:1-1}
   \begin{figure}[h]
        \includegraphics[scale=.5]{1-1.pdf}
        \caption{The density of $\alpha_2 - \alpha_1$}\label{fig:1-1}
      \end{figure}
\item
  \begin{enumerate}
  \item
    Assume that $y_i\overset{\text{\em i.i.d.}}{\sim}Beta(\alpha_y,
    \beta_y)$ with parameter $\bm\theta_y = (\alpha_y,
    \beta_y)$ and similarly, $z_i\overset{\text{\em i.i.d.}}{\sim}Beta(\alpha_z,
    \beta_z)$ with parameter $\bm\theta_z = (\alpha_z,
    \beta_z)$.

    % \begin{enumerate}
    % \item\label{2-mod-1} For the simple case,
    % we can assume that $y_1, y_2, \ldots, y_{10}$ are {\em i.i.d.}
    % uniform random variables with upper bound parameter $\bm\theta_y =
    % \mathcal U_y$ and lower bound $0$;
    % and $z_1, z_2,\ldots, z_8$ are {\em i.i.d.} uniform random variables
    % with upper bound parameter $\bm\theta_z = \mathcal U_z)$ and lower
    % bound 0. In summary,
    % \[
    % Y_i\overset{\text{\em i.i.d.}}{\sim} Unif(0, \mathcal U_y)
    % \]
    % and
    % \[
    % Z_i\overset{\text{\em i.i.d.}}{\sim} Unif(0, \mathcal U_z)
    % \] 
    % \item\label{2-mod-2} 
    %   Another sightly complicated approache is logit-normal
    %   distribution,
    %   we assume that 
    %   \[
    %   T_i = \log\left(\frac{Y_i}{1-Y_i}\right)\overset{\text{\em
    %       i.i.d.}}{\sim}\mathcal N(\mu_y, \sigma_y^2)
    %   \]
    %   where $\bm\theta_y = (\mu_y, \sigma_y^2)'$ and
    %   \[
    %   \log\left(\frac{z_i}{1-z_i}\right)\overset{\text{\em
    %       i.i.d.}}{\sim}\mathcal N(\mu_z, \sigma_z^2)
    %   \]
    %   where $\bm\theta_z = (\mu_z, \sigma_z^2)'$
    % \end{enumerate}
  \item    
    Assume that $\alpha_y, \alpha_z \overset{\text{\em i.i.d.}}{\sim}
    Negative Binomial(\xi, r_1)$ and $\beta_y, \beta_z \overset{\text{\em i.i.d.}}{\sim}
    Nagative Bionmial(\psi, r_2)$.
    Thus, 
    \[
    \mathscr P(\bm y, \alpha_y, \beta_y) = \left[\frac{(\alpha_y+r-1)!}{\alpha_y!(r_1-1)!}\frac{(\beta_2+r_2-1)!}{\beta_y!(r_2-1)!}(1-\xi)^{r_1}(1-\psi)^{r_2}\prod_{i=1}^{N_y}\frac{\Gamma(\alpha_y+\beta_y)y_i}{\Gamma(\alpha_y)\Gamma(\beta_y)(1-y_i)}\right]
    \times\]
    \[
\left[\prod_{i=1}^{N_y}y_i\xi\right]^{\alpha_y}\left[\prod_{i=1}^{N_y}(1-y_i)\psi\right]^{\beta_y}
    \]
    
    similarly,
    \[
    \mathscr P(\bm z, \alpha_z, \beta_z) =
    \left[\frac{(\alpha_y+r-1)!}{\alpha_y!(r_1-1)!}\frac{(\beta_2+r_2-1)!}{\beta_y!(r_2-1)!}(1-\xi)^{r_1}(1-\psi)^{r_2}\prod_{i=1}^{N_z}\frac{\Gamma(\alpha_z+\beta_z)z_i}{\Gamma(\alpha_z)\Gamma(\beta_z)(1-z_i)}\right]
    \times\]
    \[
    \left[\prod_{i=1}^{N_z}z_i\xi\right]^{\alpha_z}\left[\prod_{i=1}^{N_z}(1-z_i)\psi\right]^{\beta_z}
    \]

    Note: all parameters (not meta-parameters) are independent.

    The reasons to use this approach:    
    \begin{itemize}
      \item Fits $y_i$'s principle characteristic: they are proportion
        of a integer to its sum with another integer, negative binomial
        distribution's support can satifies this well.
      % \item The reason that I choose the support as $\mathbb N^+$
      %   rather than $\{0\}\cup\mathbb N^+$ is to avoid the case that
      %   $\frac{0}{0+0}$.
    \end{itemize}
    % \begin{enumerate}
    % \item For the first case in \ref{2-mod-1}, 
    %   We can assume $\mathcal U_y\sim\Gamma(\alpha, \beta)$ and 
    %   $\mathcal U_z\sim\Gamma(\alpha, \beta)$ and they are
    %   independent.
      
    %   For this simpler case, we have, $\mathcal U_y\sim\Gamma(\alpha
    %   -N, \beta)$ where $N$ is the sample size of $Y$, $\mathcal U_z$
    %   follows the similar property.

    % \item For the first case in \ref{2-mod-2}, 
    %   we can assume that d $\mu_y\sim\mathcal N(\mu_0, \sigma_0^2)$ and
    %   $\sigma_0^2\sim \Gamma^{-1}(3,1)$; $\mu_z\sim\mathcal N(\mu_1, \sigma_1^2)$
    %   $\mu_y\sim\mathcal N(\mu_1, \sigma_1^2)$ , $\sigma_1^2\sim
    %   \Gamma^{-1}(3,1)$ 
    % \end{enumerate}
  \item
    We know that
    \[
    \mathscr P(\bm y) = \sum_{\alpha_y=1}^\infty\sum_{\beta_y=1}^\infty\mathscr P(\bm y, \alpha_y, \beta_y)
    \]
    \[=
    \left[\xi^{r_1}\psi^{r_2}\prod_{i=1}^{N_y}\frac{\Gamma(\alpha_y+\beta_y)y_i}{\Gamma(\alpha_y)\Gamma(\beta_y)(1-y_i)}\right]
    \times \]
    \[
    \left[1-\prod_{i=1}^{N_y}y_i\xi\right]^{-r_1}\left[1-\prod_{i=1}^{N_y}(1-y_i)\psi\right]^{-r_2}
    \]
    we have
    \[
    \mathscr P(\alpha_y, \beta_y|\bm y) =
    \frac{(\alpha_y+r-1)!}{\alpha_y!(r_1-1)!}
    \left[\prod_{i=1}^{N_y}y_i\xi\right]^{\alpha_y}\left[1-\prod_{i=1}^{N_y}y_i\xi\right]^{r_1}\times
    \]
    \[
    \frac{(\beta_y+r_2-1)!}{\beta_y!(r_2-1)!}
    \left[\prod_{i=1}^{N_y}(1-y_i)\psi\right]^{\beta_y}\left[1-\prod_{i=1}^{N_y}(1-y_i)\psi\right]^{r_2}
    \]
    
    which is a product of two independent geometric random variables'
    probability mass function. Similarly, 
    \[
    \mathscr P(\alpha_z, \beta_z|\bm z) =
    \frac{(\alpha_z+r-1)!}{\alpha_z!(r_1-1)!}
    \left[\prod_{i=1}^{N_z}z_i\xi\right]^{\alpha_z}\left[1-\prod_{i=1}^{N_z}z_i\xi\right]^{r_1}\times
    \]
    \[
        \frac{(\beta_z+r_2-1)!}{\beta_z!(r_2-1)!}
    \left[\prod_{i=1}^{N_z}(1-z_i)\psi\right]^{\beta_z}\left[1-\prod_{i=1}^{N_z}(1-z_i)\psi\right]^{r_2}
    \]    

    For convenience, we assume that $\xi = .5$, $\psi = .5$, $r_1=10$,
    $r_2=1000$ (they are arbitrarily picked), thus, 
    \begin{small}
    \begin{verbatim}
> jags.3.1.y <- jags(data=data.3.1.y, inits=list("alpha"=rnegbin(1, 30, theta=1), 
+                    "beta"=rnegbin(1, 700, theta=1)),
+                    parameters.to.save=c("alpha", "beta"), model.file="2-1-1.bug",
+                    n.iter=1000, n.chains=2)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
   Graph Size: 17

  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
  |**************************************************| 100%
> geweke.diag(as.mcmc(jags.3.1.y))
[[1]]

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

   alpha     beta deviance 
  -1.394   -1.350   -1.380 


[[2]]

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

   alpha     beta deviance 
  0.1819   0.5033   0.5937 


> gelman.diag(as.mcmc(jags.3.1.y))
Potential scale reduction factors:

         Point est. 97.5% quantile
alpha          1.03           1.04
beta           1.04           1.04
deviance       1.03           1.03

Multivariate psrf
1.00

> jags.3.1.z <- jags(data=data.3.1.z, inits=list("alpha"=rnegbin(1, 30, theta=1), 
+                    "beta"=rnegbin(1, 700, theta=1)),
+                    parameters.to.save=c("alpha", "beta"), model.file="2-1-1.bug",
+                    n.iter=1000, n.chains=2)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
   Graph Size: 15

  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
  |**************************************************| 100%
> 
> gelman.diag(as.mcmc(jags.3.1.z))
Potential scale reduction factors:

         Point est. 97.5% quantile
alpha          1.02           1.09
beta           1.02           1.12
deviance       1.02           1.09

Multivariate psrf

1.02
> geweke.diag(as.mcmc(jags.3.1.z))
[[1]]

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

   alpha     beta deviance 
  1.3633   0.7820   0.4029 


[[2]]

Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

   alpha     beta deviance 
 -0.4858  -1.4689  -1.8225 
\end{verbatim}
\end{small}
    Both Gelman-Rubin and Geweke Diagonstic give us positive
    conclusion on this approach.
The distribution plots of $\alpha_y, \beta_y|\bm y$  and  $\alpha_y,
\beta_y|\bm y$ are shown in 
Fig.\ref{fig:2-1} and Fig.\ref{fig:2-2} Respectively.
\begin{figure}[h]
        \includegraphics[scale=.5]{2-1.pdf}
        \caption{The traceplot of $\theta_y|\bm y$}\label{fig:2-1}
      \end{figure}
\begin{figure}[h]
        \includegraphics[scale=.5]{2-2.pdf}
        \caption{The traceplot of $\theta_z|\bm z$}\label{fig:2-2}
      \end{figure}

    % \begin{enumerate}
    %   \item For model \ref{2-mod-1}, the plot is shown in ig.\ref{fig:3-1}
    %   \item For model \ref{2-mod-2}, the plot is shown in Fig.\ref{fig:3-2}
    % \end{enumerate}
  \item
    Since $\mu_y = \frac{\alpha_y}{\alpha_y + \beta_y}$ and $\mu_z =
    \frac{\alpha_z}{\alpha_z + \beta_z}$ 
    we can easily have the histogram of $\mu_y-\mu_z$, which is shown
    in Fig.\ref{fig:2-3}.
    \begin{figure}[h]
        \includegraphics[scale=.5]{2-3.pdf}
        \caption{The traceplot of $\mu_y - \mu_z$}\label{fig:2-3}
      \end{figure}    
    
    % \begin{enumerate}
    %   \item For model \ref{2-mod-1}, the histogram is shown in ig.\ref{fig:3-3}
    %   \item For model \ref{2-mod-2}, the histogram is shown in ig.\ref{fig:3-4}
    % \end{enumerate}
  \end{enumerate}
\item
  \begin{enumerate}
  \item
    First, the model is  $N_i\sim~ Poisson(\alpha + \beta t_i)$,
    undering this setting, we have the constrain that $\alpha+\beta
    t_i > 0$ for all $i = 1, \ldots, 10$. we have
    $\frac{\alpha}{\beta} < -max(t_i)$.  
    since,
    \[
    p(\bm y|\alpha, \beta, \bm t) = \prod_{i=1}^{10}\frac{(\alpha+\beta
      t_i)^{-y_i}e^{\alpha+\beta t_i}}{y_i!} 
    \]
    thus, we 
    
  \item
  \item
  \item
  \item
  \item
  \item
  \item
  \item
  \end{enumerate}

\item
  \begin{enumerate}
  \item The posterior density of $\theta$ is:
    \[ 
y    \mathscr P(\bm y) = 
    \int_{\mathbb R}\prod_{i=1}^n\pi^{-1}[1 +
    (y_i-\theta)^2]^{-1}\pi(\theta)d\theta = \pi^{-n}\int_{\mathbb R}\prod_{i=1}^n[1 +
    (\theta-y_i)^2]^{-1}d\theta 
    \]
    thus,
    \[ 
    \mathscr P(\theta|\bm y) \propto \frac{\prod_{i=1}^n[1 +
    (y_i-\theta)^2]^{-1}}{\int_{\mathbb R}\prod_{i=1}^n[1 +
    (\theta-y_i)^2]^{-1}d\theta}
    \]

    The corresponding {\tt R} code are:    
    \begin{small}
    \begin{verbatim}
require(MCMCpack)
require(arm)
require(R2jags)

post.inte.two.sample <- function(theta, obs) {
  def1 <- 1
  for (i in 1:length(obs))
    def1 <- def1/(1+(theta - obs[i])^2)
  def1
}

post.two.sample.cauchy <- function(theta, obs) {
  norm.post.0 <- integrate(post.inte.two.sample, lower=-Inf, upper=Inf, 
                           obs=obs)$value
  post.inte.two.sample(theta, obs)/norm.post.0
}

logfun1 <- function(theta, obs) {
  ## usage:
  log(post.two.sample.cauchy(theta, obs))
}

post.mean.fun.1 <- function(theta, obs) {
  theta*post.two.sample.cauchy(theta, obs)
}      
    \end{verbatim}
    \end{small}
  \item
    \begin{enumerate}
    \item
      \[ 
      \mathscr P\left(\theta|(1.5, 2.5)'\right) \propto \frac{[1 +
        (1.5 - \theta)^2]^{-1}[1 +
        (2.5 - \theta)^2]^{-1}}{\int_{\mathbb R}[1 +
        (\theta - 1.5)^2]^{-1}[1 +
        (\theta - 2.5)^2]^{-1}d\theta}
      \]
      and denominator can be computed numerically. This posterior
      density is shown in Fig.\ref{fig:4-b-i}

      \begin{figure}[h]
        \includegraphics[scale=.5]{4-b-i.pdf}
        \caption{The posterior density of $\theta$ conditional on $\bm
          y = (1.5, 2.5)'$}\label{fig:4-b-i}
      \end{figure}

    \item
      By using {\tt R}'s {\tt MCMCpack}'s {\tt MCMCMetrop1R} function,
      we can obtain the posterior samples of $\theta$:      
      \begin{small}
      \begin{verbatim}
> ## Metropolis-Hasting sampled θ for y = (1.5, 2.5)
> post.sample.b.ii <- MCMCmetrop1R(logfun1, theta.init=rnorm(1), 
+                                  thin=1, mcmc=40000, burnin=500, 
+                                  obs=c(1.5, 2.5), logfun=TRUE) 

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
The Metropolis acceptance rate was 0.72894
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
> 
> mean(post.sample.b.ii)
[1] 2.002400
> var(post.sample.b.ii)
         [,1]
[1,] 1.015655
> mean.nc <- integrate(post.mean.fun.1, lower=-Inf, upper=Inf, 
+                      obs=c(1.5, 2.5))$value
> 
> mean.nc
[1] 2
>         
      \end{verbatim}
      \end{small}
      where the MCMC mean is $2.002400$ and numerical mean is $2$.
    \item
    \end{enumerate}
  \item
    \begin{enumerate}
    \item This posterior density is shown in Fig.\ref{fig:4-c-i}
      \begin{figure}[h]
        \includegraphics[scale=.5]{4-c-i.pdf}
        \caption{The posterior density of $\theta$ conditional on $\bm
          y = (-3, -2, 1.5, 2.5)'$}\label{fig:4-c-i}
      \end{figure}
    \item
      We can obtain the MCMC and numerical result by using following
      {\tt R} commands
      \begin{small}
      \begin{verbatim}
> post.sample.c.ii <- MCMCmetrop1R(logfun1, theta.init=rnorm(1), 
+                                  obs=c(-3, -2, 1.5, 2.5),
+                                  thin=1, mcmc=80000, burnin=500, 
+                                  logfun=TRUE) 

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
The Metropolis acceptance rate was 0.80627
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
> > mean(post.sample.c.ii)
[1] -0.2505891
> var(post.sample.c.ii)
         [,1]
[1,] 3.399949
> mean.nc <- integrate(post.mean.fun.1, 
+       lower=-Inf, upper=Inf, obs=c(-3, -2, 1.5, 2.5))$value
> mean.nc
[1] -0.25
>         
    \end{verbatim}
      \end{small}
      where the MCMC mean is $-0.2505891$, the numerical mean is $-0.25$.
    \item
    \item
    \end{enumerate}
  \end{enumerate}
\item
  \begin{enumerate}
  \item
    Follow the rat tumor example setting:
    \[
    y_j \sim Bin(n_j, \theta_j)
    \]
    \[
    \theta_j\sim Beta(\alpha, \beta)
    \]
    \[
    p(\alpha, \beta)\propto (\alpha+\beta)^{-\frac{5}{2}}
    \]
    we have
    \[
    \mathscr P(\bm y, \bm\theta, \alpha, \beta) = \left[\prod_{j=1}^J
    { n_j \choose y_j}\theta_j^{y_j}(1-\theta_j)^{n_j-y_j}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha-1}(1-\theta_j)^{\beta-1}\right](\alpha+\beta)^{-\frac{5}{2}} 
    \]
   \[ =
    \left[\prod_{j=1}^J{n_j \choose y_j}\theta_j^{y_j+\alpha-1}(1-\theta_j)^{n_j-y_j+\beta-1}\right]\left(\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\right)^{J}(\alpha+\beta)^{-\frac{5}{2}} 
   \]
   and thus
   \[
   \mathscr P(\alpha, \beta, \bm y) =
   \int_0^1\int_0^1\cdots\int_0^1\left[\prod_{j=1}^J{n_j \choose
       y_j}\theta_j^{y_j+\alpha-1}(1-\theta_j)^{n_j-y_j+\beta-1}\right]\left(\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\right)^{J}(\alpha+\beta)^{-\frac{5}{2}}
   d\theta_1d\theta_2\cdots d\theta_J
   \]
   we have
   \[
   \mathscr P (\alpha, \beta|\bm y) \propto
   (\alpha+\beta)^{-\frac{5}{2}}\left(\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\right)^{J}
   \left[\prod_{j=1}^J\frac{\Gamma(y_j+\alpha)\Gamma(n_j-y_j+\beta)}{\Gamma(n_j+\alpha+\beta)}\right]
   \]
  \item
    The contour plot is shown in Fig.\ref{fig:5-1}
    \begin{figure}[h]
        \includegraphics[scale=.5]{5-1.pdf}
        \caption{The contour plot of $P(\alpha, \beta|\bm y)$}\label{fig:5-1}
      \end{figure}



    The posterior density plot of $\alpha$ and $\beta$ and the MCMC
    samples' density of the posterior distribution are shown in Fig.\ref{fig:5-2}
    \begin{figure}[h]
        \includegraphics[scale=.5]{5-2.pdf}
        \caption{The marginal density plot of $\mathscr P(\alpha, \beta|\bm y)$}\label{fig:5-2}
      \end{figure}
  \item
    Following the suggestion from Gelman, we draw random samples from
    joint posterior distribution $\mathscr P(\alpha, \beta,
    \bm\theta|\bm y)$ with following density function:
    \[\mathscr P(\alpha, \beta,
    \bm\theta|\bm y)\propto 
    (\alpha+\beta)^{-\frac{5}{2}}\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha+y_j-1}(1-\theta_j)^{n_j-y_j+\beta-1}
    \]
    Fig.\ref{fig:5-3} shows the posterior density of $\theta_j$'s,
    none of $\frac{y_j}{n_j}$ out of the boundaries of the estimated
    distribution 95\% interval:
    \begin{small}
    \begin{verbatim}
> for (j in 1:10)
+   if (max(θ.sims[,j]) < y.Js[j]/n.Js[j] | min(θ.sims[,j]) > y.Js[j]/n.Js[j])
+   print (paste(j, "observed value not in the HPD"))
>   
> 
    \end{verbatim}
    \end{small}
    \begin{figure}[h]
        \includegraphics[scale=.5]{5-3.pdf}
        \caption{The posterior density plot of $\theta_j$'s}\label{fig:5-3}
      \end{figure}
  \item
    It simples using emprirical estimation:
    \begin{verbatim}
> print(quantile(rowMeans(θ.sims), c(0.025, 0.95)))
     2.5%       95% 
0.1731441 0.2172680       
    \end{verbatim}
  \item
    
  \item

  \end{enumerate}
  
\end{enumerate}

\end{document}






































